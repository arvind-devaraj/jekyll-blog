---
layout: post
title:  "Hadoop and Map Reduce"
date:   2016-02-11
category: technology
tags: [css, html]

# Author.
author: Arvind
---
![picture](https://placehold.it/250x150)
 

 Hadoop is a MapReduce framework that enables processing large datasets in parallel, on clusters of commodity hardware. This is cheaper, as it’s a open source solution that can run on commodity hardware while handling petabytes of data. It’s faster on massive data volumes as data processing is done in parallel.

A complete Hadoop MapReduce based solution may have following layers.

1.Hadoop core- HDFS (Hadoop Distributed File System)
This is the data storage system where data is splitted into large files ~64 or 128MB blocks and saved. This can scale to 1000s of nodes and is inspired by Google File System that resolved the problem of indexing the web. In order to cater for fault tolerance, by default, HDFS keep 3 replicas of same data that it stored. Hence there can be integrity violations on the data set at times.
2.MapReduce API
Allows job-based(batch) parallelizable processing, across data in HDFS. This API enables auto-parallelize for huge amounts of data while preserving fault tolerance and adding high availability. Also enables computing logics to come to data, rather than data travelling across the network to be operated on. Mapreduce algorithm expects hardware failures in the used commodity hardware, so have automatic retry is built-in.